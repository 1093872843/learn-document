# 调度管理

通常情况下啊，我们不需要关心 Pod 会被分配到具体哪个节点上，只需要能够访问即可。但是在生产环境中，可能出现 Pod 需要运行在指定的节点，比如 某些应用只有在硬件更好的条件下才能运行，由此需求 k8s 提供了调度策略。

- 节点选择器
- 节点亲和性
- Pod 亲和性和反亲和性
- 污点和容忍
- nodeName

## 节点选择器

节点选择器通过节点标签和 pod 的 nodeSelector 属性将 Pod 调度到具有特定标签的节点上

```linux
# 查看节点标签
kubectl get node --show-labels

# 新增节点标签
kubectl label nodes <节点名称> <标签名>=<值>

# 新增Pod标签
kubectl label pod <pod名称> <标签名>=<值>

```

pod 设置标签 （nodeSelector）

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web
spec:
  nodeSelector:
    diskType: asd # <标签名>: <值>
  containers:
    - name: web
      image: nginx:1.23
```

如果 Pod 无法找到匹配节点，则会一直处于 pending 状态，直到找到为止。

## 节点亲和性

它的功能和节点选择器类似，但是功能更为精确和强大。

- 支持逻辑符表达能力

  In：包含指定的标签键值对

  NotIn: 不包含指定的标签键值对

  Exists: 存在指定的标签键，无论值是什么

  DoesNotExist: 不存在指定的标签键，无论值是什么

  Gt: 指定标签键的值（整数或浮点数）大于指定的值

  Lt: 指定标签键的值（整数或浮点数）小于指定的值

- 支持硬性条件（requiredDuringSchedulingIgnoredDuringExecution）和偏好条件（preferredDuringSchedulingIgnoredDuringExecution）

  硬性条件表示只能被调度到条件符合的节点，偏好条件表示如果找不到符合条件的节点，允许调度到接近条件的节点

下述包含硬性条件的 Pod 确保该 Pod 会被调度到具有标签 “disktype=ssd”的节点上，如果不存在匹配，则该 Pod 会一直处于 pending 状态。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hard-constraint-pod
spec:
  containers:
    - name: my-container
      image: nginx
  affinity: # 定义亲和性规则
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions: # 匹配表达式
              - key: disktype #标签键
                operator: In # 操作符
                values: # 标签键对应的值，可以包含多个
                  - ssd
```

包含偏好条件的 Pod 定义

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: preferred-affinity-pod
spec:
  containers:
    - name: my-container
      image: nginx
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100 #指定偏好的权重，多偏好条件匹配时，优先匹配权重高的Pod。
          preference:
            matchExpressions:
              - key: topologyZone
                operator: In
                values:
                  - beijing
```

## Pod 亲和性和反亲和性

节点亲和性用于控制 Pod 与节点之间的关系，而 Pod 亲和性和反亲和性用于控制同一个节点内 Pod 和 Pod 之间的关系。

Pod 亲和性表示希望两个 Pod 尽可能靠近，比如为了减低延迟或高效的数据传输；Pod 反亲和性表示希望两个 Pod 尽可能远离，比如避免多个副本 Pod 调度到同一节点，提高可用性。

它同样支持硬性条件和偏好条件两种类型

下述为定义 Pod 亲和性的例子，描述了一个名为 web 的 Pod,并希望它尽可能地和一个标签键包含 db 的容器放置在同一台节点。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: affinity-pod
spec:
  containers:
    - name: web
      image: nginx:1.23
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions: #匹配表达式
                - key: app
                  operator: In
                  values:
                    - db
            topologyKey: "kubernetes.io/hostname" # 指定拓扑域的键，"kubernetes.io/hostname"表示基于节点的主机名作为拓扑域，即一个节点是一个拓扑域
```

下述描述是使用了硬性条件定义了 Pod 反亲和性的例子,表示避免将 Pod 调度到具有标签“app=nginx”的 Pod 节点上，

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: web
          image: nginx:1.23
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - nginx
              topologyKey: "kubernetes.io/hostname"
```

## 污点与容忍

节点通过设置污点（Taint）来表示该节点有某种限制或不适合大多数 Pod 调度。Pod 通过容忍（Toleration）来声明它可以接受这些限制，并允许被调度到具有该污点的节点上。

使用场景：污点和容忍机制主要用于避免 Pod 调度到不适合的节点，或者将特定的 Pod 限制到只有某些具有特殊用途的节点

### 污点

节点设置一个或者多个污点

```linux
# kubectl taint node <节点名称> <key>=<value>:[effect]
kubectl taint node k8s-master k9s-worker gpu=nvidia:NoSchedule
```

- key

  污点的名称

- value

  污点的值，可选

- effect

  污点对 Pod 调度的影响，有三个枚举值可选

  - NoSchedule: 不会调度未配置容忍这类污点的 Pod 到该节点上。
  - PreferNoSchedule: 尽量不会调度未配置容忍这类污点 Pod 到该节点上。
  - NoExecute: 不会调度未配置容忍这类污点 Pod 到该节点上，并且驱逐节点上没有容忍这类污点的 Pod

举例如下：

现在有个一个包含六个节点的集群，分别为 Node1~Node6,其中 1~4 为常规业务，5，6 为机器学习业务，为了保证常规业务不会被调度到 5，6 机器上占用资源，需要设置污点如下。

```linux
#设置污点,这样一来，这两个节点将不会调度未配置容忍的Pod
kubectl taint node Node5 Node6 gpu=nvidia:NoSchedule

#查看污点
kubectl describe node Node5 Node6 |grep Taint

#移除污点
kubectl taint node <节点名称> <key>-
```

### 容忍

在 Pod 定义容忍规则，允许机器学习程序被调度到 Node5 和 Node6 节点上

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tensorflow-pod
spec:
  tolerations:
    - key: gpu
      operation: Equal
      value: "nvidia"
      effect: NoSchedule
  containers:
    - name: tf
      image: tensorflow/tensorflow:latest
```

上述描述中，tolerations 定义了容忍规则，表示该 Pod 允许被调度到具有键值 “gpu = nvidia”且效果为“NoSchedule”污点的节点上

需要注意的是，容忍的作用并不会让该 Pod 必须调度到相应污点的节点上，而是告知调度器可以被调度，所以上述描述的 Pod 仍有可能会被调度到 Node1~Node4 节点，为了避免这种情况，可以使用节点选择器或者节点亲和性来实现这一点，确保 5，6 节点已经设置标签“gpu = nvidia”，修改配置如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tensorflow-pod
spec:
  tolerations:
    - key: gpu
      operation: Equal
      value: "nvidia"
      effect: NoSchedule
  nodeSelector:
    gpu: "nvidia"
  containers:
    - name: tf
      image: tensorflow/tensorflow:latest
```

## NodeName

nodeName 是 Pod 中的一个字段，用于指定 Pod 被分配的节点名称，如果该字段不为空，那么调度器将跳过调度节点，直接分配 Pod 到指定节点上。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  nodeName: k8s-work
  containers:
    - name: tf
      image: tensorflow/tensorflow:latest
```

需要注意的是,nodeName 会将该 Pod 始终绑定到指定节点上，即使该节点发生故障，Pod 也不会重新调度到其他节点上。因此，nodeName 常用于测试和调试场景。